\chapter{基于单目图像的智能手机手部全感知算法}
\label{cha:method}

在这一章中，将会介绍
\section{问题定义}
我们通过智能手机前置摄像头结合斜面镀膜的直角三棱镜捕获有一定视差的对称的用户操作手机时手部的图像与反射像。以水平视角的经过三棱镜反射与未经过反射的对称的手部RGB或者IR图像构成的视频流作为输入，我们希望通过计算机视觉算法，从图像中检测与交互相关的手部信息，例如持握手势、点击手指，指尖位置，手指的点击角度等。基于这些额外的信息，再进一步进行交互设计。

\section{硬件设计}


\begin{figure}
\centering
\includegraphics[width=5in]{figures/method/guanglu.png}
\caption{前置摄像头成像光路图}
\label{fig:guanglu}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=5in]{figures/method/capture.png}
\caption{RGB/模拟IR环境下摄像头采集的图像}
\label{fig:capture}
\end{figure}

我们借鉴了HandSee\cite{Yu:2019:HEF:3290605.3300935}中借助三棱镜的轻量化硬件设计的想法，重新进行了硬件设计。具体地，我们将斜面镀膜的直角三棱镜放置于前置摄像头上方，它将触摸屏上方空间的图像反射至前置摄像头中，光路图的设计如图\ref{fig:guanglu}所示。由于反射次数与反射面介质不同，区域1、2、3中的成像性质也会有不同（如图\ref{fig:capture}所示），其中区域1为通过一次镜面反射直接进入摄像头的光线，成像质量最为纯净可靠；区域2成像较暗且会混入屏幕光，质量最差；区域3通过三棱镜侧面镜面与三棱镜底面两次反射，成像会有一定的系统色彩偏差。因此，我们主动地抛弃了2、3区域，仅使用区域1的图像作为后续算法的输入。

\begin{figure}
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[height=1.8in]{figures/method/shell.png}
\caption{硬件设计图}
\label{fig:hardware_design}
\end{minipage}%
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[height=1.8in]{figures/method/shiwutu.png}
\caption{实物图}
\label{fig:hardware_real}
\end{minipage}
\end{figure}

我们使用了Samsung Galaxy S9+作为实验机型，我们通过3D建模与3D打印技术打印手机外壳用以固定三棱反射镜，在三棱反射镜左侧放置一枚LED灯，并为该LED灯设计了独立的电池盒与开关（如\ref{fig:hardware_design}所示，实物图如\ref{fig:hardware_real}）。我们的硬件设计的一个直接的优势是它可以模拟RGB摄像头与IR摄像头（红外摄像头）两种工作模式：在自然光环境下，关闭LED灯，摄像头捕捉自然光环境下的RGB图像；在黑暗环境下打开LED灯，若周围无其他光源，则该LED灯为唯一的主动光源，此时捕获的图像等价于IR摄像头通过窄带滤光片捕获到的由单一红外光源形成的红外图像（如\ref{fig:capture}右图所示）。对于后者，由于此时仅有唯一的主动光源，摄像头捕获到的图像具有背景纯净、质量较高的特点，易于后续的处理，具有更大的潜力。

经过测量，该实验系统的视野范围（FoV）为56度 $\times$ 33度，水平视野范围可覆盖至手机左右侧边中点之间构成的张角的区域，垂直视野能覆盖正常操作的手部范围，因此能有效地捕捉正常操作时的手部图像。

\section{算法流水线}

我们的算法流水线如图\ref{fig:algorithm}，算法的输入为摄像头捕捉的视频流构成的单目RGB/IR图像序列，输出为握姿信息、指尖位置信息等通用特征以及下降模式等针对特定应用的特征。

算法流水线如下：首先，我们使用MobileNetV2模型对视频流中的视频帧进行握姿分类，实时地检测用户当前的握姿；在RGB单目图像下，我们使用MobileNetV2-SSD模型进行指尖区域的检测；在IR单目图像下，我们提出了一种基于握姿与边缘曲率统计特征的轮廓分类与指尖识别方法；最后，我们通过运动模式匹配与检测算法，检测指尖某种模态的运动过程（如下落、上升）。

与HandSee相比，我们的算法更具有通用性与高效性。其一，对于HandSee的完整算法流程，一个观察是，HandSee中预处理（13ms）、双目视觉匹配（10ms）、CNN手部分割（4ms）三个部分占总流程时长的80\%以上，且在服务器端计算需要花费27ms时间，无法满足移动端实时计算的需求；其二，HandSee的算法流程直接的输出是手部深度图的重构信息与通过简单形态学检测找出的指尖信息，而我们的算法流程可以得到包括握姿、优化的指尖、下降模式等更高级的语义信息。这侧面体现出我们的算法流程的科学性与合理性。

\begin{figure}
\centering
\includegraphics[width=5in]{figures/method/algorithm.pdf}
\caption{基于单目RGB/IR的全手行为感知算法流程图}
\label{fig:algorithm}
\end{figure}

\section{握姿检测模型}
\label{cha:grip}

\subsection{卷积神经网络}
卷积神经网络是图像分类任务中最普遍应用的神经网络模型。由卷积层、池化层构成，其基本原理如下：

\textbf{卷积层：}

首先定义二维卷积操作，对于$W \in R_{h,w}$，其每一个像素记作$W_{i,j}$；$s \ in R_{2k+1,2k+1}$，其每一像素记作$s_{i,j}$，定义$W$与$s$的卷积为$O = W \odot s$，其中，

\begin{equation}
    O_{m,n} = \sum\limits_{i = -k}^k \sum\limits_{j = -k}^k W_{m+i,n+j} \cdot s_{2k+1-i, 2k+1-j}
\end{equation}

有了以上的卷积操作定义，我们可以进一步定义多输入、多输出通道的卷积层，设输入图像矩阵$I \in R_{h \times w \times d_i}$，其中h为图像矩阵的高度，w为图像矩阵的宽度，$d_i$为图像矩阵的输入（色彩）通道数，它的每一个像素记作$I_{i,j,k}$，卷积核$s \in R_{d_i \times k \times k \times d_o}$，每一位置权重记作$s_{i,j,k,l}$，输出的矩阵为$O \in R_{h \times w \times d_o}$。

\begin{equation}
    O_{m,n,p} = \sum\limits_{c = 0}^{d_i - 1} \sum\limits_{i = -k}^k \sum\limits_{j = -k}^k I_{i,j,c} \cdot s_{c, 2k+1-i, 2k+1-j, p}
\end{equation}



步长为l：
\begin{equation}
    O_{m,n,p} = \sum\limits_{c = 0}^{d_i - 1} \sum\limits_{i = -k}^k \sum\limits_{j = -k}^k I_{m \cdot l+i,n \cdot l+j,c} \cdot s_{c, 2k+1-i, 2k+1-j, p}
\end{equation}


\textbf{池化层：}

\begin{equation}
    O_{m,n,p} =  {1 \over k^2} \sum\limits_{i = -k}^k \sum\limits_{j = -k}^k I_{m \cdot l+i,n \cdot l+j,p}
\end{equation}

整个结构

卷积层能很好地捕捉图像的局部特征，通过卷积层的堆叠，可。。。高层语义特征


\subsection{MobileNetV2模型结构}
由于MobileNetV2计算效率的高效性，我们使用MobileNetV2 作为握姿检测的前馈特征提取网络。MobileNetV2的原理与网络结构如下。

\textbf{深度可分离卷积：}深度可分离卷积的核心思想是将传统卷积层分解成横向与纵向两个卷积层。第一层为一个单卷积层，它被应用与输入的每个维度，第二层为$1 \times 1$的多通道卷积层（也称为点卷积）。具体地，设输入$L_i \in R_{h_i \times w_i \times d_i}$，传统卷积核$K \in R_{k \times k \times d_i \times d_j}$，卷积操作后得到输出$L_j \in R_{h_j \times w_j \times d_j}$；而对于深度可分离卷积，第一层卷积核$K_1 \in R_{k \times k \times d_i}$，它与$L_i$卷积后得到$L_{middle} \in R_{h_j \times w_j \times d_i}$，第一层卷积核$K_2 \in R_{1 \times 1 \times d_i \times d_j}$，卷积操作后得到输出$L_j \in R_{h_j \times w_j \times d_j}$。

\textbf{线性瓶颈模块与反残差层：}理论表明\cite{Sandler_2018}，ReLu层会对模块的输出特征造成破坏，因此线性瓶颈模块在借鉴了传统残差模块的瓶颈模块（ResNet）的同时，改变了其特征压缩/扩张策略，并用深度可分离卷积替换其传统卷积层，形成反残差层。反残差层的网络结构如表\ref{tab:InverseResidual}。

\begin{table}[htbp]
  \centering
  \caption{反残差层的网络结构}
  \label{tab:InverseResidual}
    \begin{tabular}{p{80pt}p{120pt}p{80pt}}
      \toprule
      Input & Operator & Output\\
      \midrule
      $h \times w \times k$  &  $1 \times 1$ conv2d, ReLU6 &   $h \times w \times (tk)$ \\
      $h \times w \times (tk)$  &  $3 \times 3$ dwise s, ReLU6 & ${h \over s} \times {w \over s} \times (tk)$ \\
      ${h \over s} \times {w \over s} \times (tk)$ &   linear $1 \times 1$ conv2d & ${h \over s} \times {w \over s} \times k'$\\
      \bottomrule
    \end{tabular}\\[5pt]
\end{table}

其中，t表示扩张因子，其作用是从输入中扩展更多通道的特征；s表示depth-wise卷积操作的步长，当$s=1$时输出时加上残差，结构类似于ResNet中的残差层，当$s=2$功能类似于池化层。

进一步地，以上述反残差层为基本单元，MobileNetV2的网络结构如表\ref{tab:MobileNetV2}


\begin{table}[htbp]
  \centering
  \caption{MobileNetV2网络结构}
  \label{tab:MobileNetV2}
    \\[5pt]
    \begin{tabular}{p{80pt}p{80pt}p{40pt}p{40pt}p{40pt}p{40pt}}
      \toprule
      Input & Operator & t & c & n & s\\
      \midrule
      $224^2 \times 3$  &  conv2d & - & 32 & 1 & 2 \\
      $112^2 \times 32$  &  bottleneck & 1 & 16 & 1 & 1 \\
      $112^2 \times 16$  &  bottleneck & 6 & 24 & 2 & 2 \\
      $56^2 \times 24$  &  bottleneck & 6 & 32 & 3 & 2 \\
      $28^2 \times 32$  &  bottleneck & 6 & 64 & 4 & 2 \\
      $14^2 \times 64$  &  bottleneck & 6 & 96 & 3 & 1 \\
      $14^2 \times 96$  &  bottleneck & 6 & 160 & 3 & 2 \\
      $7^2 \times 160$  &  bottleneck & 6 & 320 & 1 & 1 \\
      $7^2 \times 320$  &  conv2d $1\times1$ & - & 1280 & 1 & 1 \\
      $7^2 \times 1280$  &  avgpool $7\times7$ & - & - & 1 & - \\
      $1^2 \times 1280$  &  conv2d $1\times1$ & - & k & - & - \\
      \bottomrule
    \end{tabular}\\[5pt]
\end{table}


具体地，对于握姿检测任务，设待区分的握姿集合为$G=\{G_1,G_2,...,G_m\}$，对于单独的视频帧$f_k$，设通过MobileNetV2提取的图像特征向量为$v_k=MobileNet(f_k) \in R_{n}$，设$W \in R_{n \times m}, b \in R_{m}$，则$f_k$对于$G$的概率向量为$p = Softmax(v_i \cdot W + b)$，其中$Softmax(x)_i = {{exp(x_i)} \over {\sum_i exp(x_i)}}$。设q为$f_k$的one-hot事实标注，则交叉熵损失函数可表示为$H(q,p)=-\sum_i q_i logp_i$。

\subsection{参数优化与训练策略}

使用Adam优化器\cite{2014arXiv1412.6980K}对网络参数进行优化，参数更新规则如公式\ref{adam}。

\begin{equation}
\label{adam}
\left\{\begin{array}{l}
    g_t \leftarrow \nabla_\theta H_t(\theta_{t-1})\\[0.3cm]
    m_t \leftarrow \beta_1 \cdot m_{t-1} + (1-\beta_1) \cdot g_t\\[0.3cm]
    v_t \leftarrow \beta_2 \cdot v_{t-1} + (1-\beta_2) \cdot g_t^2\\[0.3cm]
    \hat{m_t} \leftarrow {m_t \over {1- \beta_1^t}}\\[0.3cm]
    \hat{v_t} \leftarrow {v_t \over {1- \beta_2^t}}\\[0.3cm]
    \theta_t \leftarrow \theta_{t-1} - {\alpha \cdot {\hat{m_t}} \over {(\sqrt{\hat{v_t}} + \epsilon)}}
\end{array}\right.
\end{equation}

除此之外，还可以以相邻的k帧作为输入进行预测。设对于相邻的k帧$f_1,f_2,\cdots,f_k$，它们的特征向量$v_i=MobileNet(f_i),i=1,2,\cdots,k$，将它们拼接后得到长度为nk的特征向量$v=[v_1;v_2;\cdots;v_{nk}]$，类似地可得该视频帧序列对于$G$的概率向量为$p = Softmax(v \cdot W + b)$，与单帧情况类似，训练时计算交叉熵损失，并使用使用Adam优化器对网络参数进行优化。

\section{指尖识别算法}
\label{cha:fingertip}
单目RGB图像与单目IR图像特征差异较大，前者往往需要处理诸如背景干扰、光照条件的一致性等问题，而后者往往可以得到清晰、纯净的手部图像。因此，我们对于单RGB图像与单IR图像，分别提出了两种不同的指尖检测算法策略。对于前者，我们使用基于MobileNetV2的Single Shot Detection（
SSD）神经网络模型进行检测；对于后者，我们设计了一种基于基于形态学特征的指尖检测算法。

\subsection{基于单RGB图像的指尖检测模型}

\subsubsection{Single Shot Detection}

\subsubsection{MobileNetV2 SSD}
对于单RGB图像，我们使用以MobileNetV2作为前馈特征提取网络的SSD（Single Shot Detection）神经网络模型直接对指尖区域进行提取。我们按照\inlinecite{Sandler_2018}与\inlinecite{Huang2017Speed}的设定进行网络的构建。首先将传统的SSD网络中所有的卷积层替换成深度可分离卷积，我们将替换过后的SSD网络称为SSDLite。对于MobileNetV2，我们将MobileNetV2第15层的输出（输出的步长为16）作为SSDLite第一层的输入，将MobileNetV2最后一层的输出（输出步长为32）接至SSDLite第二层及之后。


对于multibox-classifier，其损失函数可表示为：

\begin{equation}
    L(x,c,l,g)={1 \over N}(L_{conf}(x,c) + \alpha L_{loc}(x,l,g))
\end{equation}

其中$L_{loc}$为预测框的位置损失函数，$L_{conf}$为置信度损失函数：

\begin{equation}
    L_{loc}(x,l,g)= \sum_{i \in Pos}^N \sum_{m \in \{cx, cy, w, h\}} x_{ij}^k smooth_{L1}(l_i^m-\hat{g}_j^m)
\end{equation}

\begin{equation}
    L_{conf}(x,c)=- \sum_{i \in Pos}^N x_{ij}^p log(\hat{c}_i^p) - \sum_{i \in Neg} log(\hat{c}_i^0), \hat{c}_i^p = {exp(c_i^p) \over {\sum_p exp(c_i^p)}}
\end{equation}

其中$x_{ij}^p \in {0,1}$表示第i个候选框与第j个事实框关于第p类是否匹配（IOU>0.5）；c表示某个候选框预测的分类置信度，其中第0类表示背景；l为预测框；g为事实框。

\subsection{基于单IR图像的指尖识别算法}
对于单IR图像，由于其背景纯净，直接通过阈值分割的方法提取手部区域是十分有效的，因此在这一节，我们使用基于形态学特征的指尖检测算法对指尖进行检测。与\inlinecite{Chen:2014:AIT:2642918.2647392}和\inlinecite{Yu:2019:HEF:3290605.3300935}单纯依赖区域轮廓的简单几何特征（例如到中心点的距离、局部曲率）进行指尖预测不同，我们对指尖检测算法进行了基于握姿、轮廓形态、轮廓大小、轮廓位置的更细致的设计。

\subsubsection{边缘检测与连通区域检测}

边缘检测：我们使用Sobel算子对图像的边缘进行检测，对于图像A，  分别使用横向、纵向的Sobel算子对图像进行滤波，分别得到A的边缘$G_x$与纵向边缘$G_y$,，如下

\begin{equation}
    G_x = \left[
    \begin{matrix}
        -1 & 0 & +1 \\
        -2 & 0 & +2 \\
        -1 & 0 & +1
    \end{matrix}
    \right] \cdot A,
    G_y = \left[
    \begin{matrix}
        +1 & +2 & +1 \\
        0 & 0 & 0 \\
        -1 & -2 & -1
    \end{matrix}
    \right] \cdot A
\end{equation}

\subsection{基于边缘曲率特征的区域特征检测}

\subsection{基于KNN的区域匹配与指尖匹配算法}

对于视频帧f，首先通过阈值分割提取出手部区域$Regions_f=\{R_1, R_2, \cdots, R_n\}$，其中$R_i$表示第i个连通区域（过滤掉面积小于一定数值的噪声区域）。紧接着通过轮廓检测算法提取每一个连通区域的轮廓，得到轮廓集合$Edges_f = \{e_i = edge(R_i), i=1,2,\cdots,n\}$。我们首先根据轮廓的形态、大小、位置以及握姿的信息将每个轮廓划分为以下类别之一：左持握手拇指、右持握手拇指、双手拇指、持握手手指（非拇指）、操作手、其他（可能是非手部区域）。

对于手部形态特征，我们采用了一种创新的基于边缘曲率分布的区域特征度量方法。对于曲线C上的点p，C在p处的曲率为$lim_{\Delta s \to 0} {{\Delta \alpha}\over{\Delta s}}$，可将其转化为等价的离散形式：

\begin{equation}
    curve(p,k)=lim_{\Delta s \to 0} {{\Delta \alpha}\over{\Delta s}} = {|y''| \over {(1+y'^2)^{1.5}}} = {2h \over{\sqrt{h^2+d^2}\cdot 2d}}
\end{equation}

其中，h为$p_i$到直线$p_{i-k}p_{i+k}$的有向距离，2d为$||p_{i-k}-p_{i+k}||$，即$p_{i-k}$到$p_{i+k}$的距离。由于选取的离散点为相邻点或等间隔点，这里的曲率特征也可近似替换为偏转角度特征$\hat{curve}(p)={h \over 2d}$。我们通过曲率特征不同尺度的堆叠来刻画轮廓在某点处及其周围的特征：$feat(p_i) = \{curve(p_i,k), k \in K\}$，其中K为我们感兴趣的尺度集合，一般地，可取$K=\{1, 2, 4, 8, 16, 32, 64\}$。对于某个轮廓的形态特征$F_e$，可以将其表示为其中所有点的曲率特征的集合：$F_e = \{feat(p), p \in e\}$。由于轮廓的大小与尺度的不同，需要对特征在空间维度与曲率大小维度进行归一化。归一化后，所有的轮廓的特征均可表示为一个大小相等的矩阵。由于特征具有空间上的轮换不变性，两个特征矩阵的距离可表示为
\begin{equation}
    dist(M_{e1}, M_{e2}) = min\{||M_{e1}[l+1,\cdots,n,1,\cdots,l]-M_{e2}||_F, l = 1,\cdots,n\}
\end{equation}
其中$M_{e1}[l+1,\cdots,n,1,\cdots,l]$表示$M_{e1}$在空间维度上的一个轮换排列。

对于握姿信息，通过上节的MobileNetV2网络分类获得；对于面积、位置信息，直接计算轮廓所包围区域的面积、质心；对于边缘位置信息，直接统计轮廓在图像边缘上的点的数目。随后，可以通过人工设计的经验规则及形态特征匹配的数据对区域进行分类：

\begin{equation}
    p(e) = 0.5\times p_{rules} + 0.5 \times p_{shape}=0.5 \times M_1 \cdot feature(e) + 0.5 \times KNN(e, G)
\end{equation}

其中$M_1$为权值矩阵，$feature(e)$为特征向量，$G$为有标注的小样本数据集，$KNN(e, G)$为与e最接近的k个样本投票的预测值。

基于轮廓分类与手部形态特征，可进行细粒度的指尖检测。对于轮廓e，设$P=\{p_1, p_2, \cdots, p_t\}$为e中的曲率极值点集，对于极值点p，对以p为中心大小为d的窗内的分布在已知的指尖模板集合G中进行匹配，若二者距离小于阈值，则将p加入指尖候选集。进一步，可通过轮廓的类别拒绝指尖候选集中不合理的指尖。



\section{运动模式的匹配与检测}
\label{cha:motion}
设$S_k = \{p_{k,1}, p_{k,2}, ... , p_{k,m_k}\}$为第k个视频帧所对应的指尖候选点集，则我们需要检测的是视频流所对应的候选点集序列$S_0, S_1, ... , S_k$中是否存在满足下降运动模式$P$的点的序列。我们首先来定义下降运动模式$P$，我们称点序列$s_p=\{p_{a_i,b_i}|i=1,2,...,m; a_1 < a_2 < ... < a_m\}$满足运动模式$P$当且仅当$$\forall i \in \{1,2,...,m-1\}, 0 < y_{p_{i+1}} - y_{p_i} < \epsilon_1, |x_{p_{i+1}} - x_{p_i}| < \epsilon_2$$

其中$y_{p_{i+1}} - y_{p_i} > 0$保证了下降趋势，$y_{p_{i+1}} - y_{p_i} < \epsilon_1$ 和 $|x_{p_{i+1}} - x_{p_i}| < \epsilon_2$保证了候选点的一致性。

进一步地，可以使用动态规划的策略对问题进行求解：

\begin{algorithm}
\caption{使用动态规划求解最长下降子序列长度}
\begin{algorithmic}
\Require 相邻的k帧的指尖候选点集$S_1, S_2, \cdots, S_k$，其中$S_i = \{p_{i,1}, p_{i,2}, ... , p_{i,m_i}\}$
\Ensure 最长子序列长度

定义函数$f: \bigcup_i S_i \rightarrow N^*-\{0\} $
\For{i = 1, 2, $\cdots$, k}

    \For {$p \in S_i$}
    
        \State $f(p) \leftarrow 1$
        
        \For {$q \in \{p_{u,v}, u < i\}$}
        
            \If {$0 < y_{p} - y_{q} < \epsilon_1, |x_{p} - x_{q}| < \epsilon_2$}
            
                \State $f(p) \leftarrow max(f(p), f(q)+1)$;
                
            \EndIf
        
        \EndFor
        
    \EndFor
    
\EndFor

最长子序列长度 = $max(\{f(p), p \in \bigcup_i S_i\})$

\end{algorithmic}
\end{algorithm}

对于相邻的k帧的指尖候选点集$S_k = \{p_{k,1}, p_{k,2}, ... , p_{k,m_k}\}$，设其满足下降运动模式$P$的最长子序列$s_p$长度为$l={max(L(s_p)) \over k} $，则定义置信系数$\lambda={l \over k}={max(L(s_p)) \over k}$。当$\lambda > \lambda_0$，可认为在该帧区间内发生指尖下降行为。

由于视频帧有一定概率出现模糊，导致指尖候选集的结果错误或不完整，因此需要一定的容错机制，这在算法中通过不强制限定帧间连续来实现。

\chapter{基于智能手机手部全感知的被动交互技术}

\section{误触检测}
误触是人们使用智能手机时频繁出现的问题，受限与屏幕大小、屏幕传感原理等因素，误触在人们使用智能手机时频繁发生，极大地影响了智能手机的使用效率与使用体验。

我们将常见的误触场景总结为以下四类：
\begin{itemize}
    \item 用户有意识的手部点击行为引起的非目标位置的电容信号造成的误触，例如曲面屏幕上边缘误触、非舒适点击引起的误触等；
    \item 用户无意识的手部非点击行为引起的电容信号造成的误触，例如用户使用手机时（例如看视频）的无意识触碰；
    \item 其他导电介质触碰产生的电容信号造成的误触，例如下雨或者湿手使用手机时屏幕上的水对输入信号的干扰、放在口袋中的误触等；
    \item 由点击精度引起的目标选择的误触，例如打字、手指移动光标等。
\end{itemize}

最后一种情况的优化涉及到针对应用的先验模型、用户心理模型等知识，而与手部行为无直接关系，故在此不做详细讨论。而针对前三种情况，我们希望在已知手部行为的图像信息的情况下，这三类误触问题均能得到较好的解决。为了实现这三类误触问题的检测，从直接获取的手部行为信息的角度，我们需要检测以下三种信息：1、用户是否持握手机（是否正在使用），握姿如何；2、某次电容信号的产生是否由一个“指尖下落的过程”完成；3、对于一次有用户有意识的手部点击，哪些触点是用户“有意识点击”产生的，哪些是由误触产生的。

对于上述的三类信息，均可以通过\ref{cha:method}描述的手部全感知流程中直接或间接获取。

对于用户是否持握手机检测以及握姿检测，使用MobileNetV2为前馈特征提取网络的分类器进行分类。特别地，与\ref{cha:grip}中的握姿分类不同的是，目标类别中需加入一个无手部持握的图像类别（或纯环境类别），相应的需对该类别的数据进行采集。

对于指尖下落过程的检测，无论是对于RGB图像还是IR图像，首先通过\label{cha:fingertip}指尖检测算法检测出每一帧图像的所有指尖候选点，然后对于视频流所对应的指尖候选点集序列应用下降模式匹配\label{cha:motion}方法。若对于连续的k帧，最长下降序列长度为k'，满足$k'>\lambda \cdot k$，其中$\lambda$为置信度阈值，则可判断此时有指尖下降行为发生。

对于触点检测，我们综合了握姿检测、指尖检测与原始图像经过MobileNetV2后输出的特征向量三种信息进行判断。我们将握姿解码成one-hot向量，将指尖标注的图像（仅有对应位置的标注，无原始图像）降采样后展平成一维向量，然后将他们与原图像的特征向量进行拼接，随后通过一个简单的全连接层线性分类器对其进行分类。在此，我们将触点检测任务分为对比触点检测与独立触点检测。对比触点检测需要在一对触点（其中一个为正确触点，一个为错误触点）中选择正确触点；独立触点检测需要判断给定的一个触点是否为正确触点。显然，后者的难度要大于前者：对于前者，我们希望模拟的是边缘误触的情况；对于后者，我们希望模拟由屏幕上的干扰信号引起误触的情况。

最终，我们完整的误触检测逻辑与流程如图\ref{fig:mistouch_flow}所示。

\begin{figure}[h]
  \centering
  \includegraphics[width=6in]{figures/method/mistouch_flow.pdf}
  \caption{误触检测流程图}
  \label{fig:mistouch_flow}
\end{figure}

\section{Hand-Around检测}

Hand-Around检测的目标是通过单目图像，区分以下三种模态：Hand-On，用户持握手机或正在操作手机，摄像头能捕获到近场的手部图像；Hand-Around，用户的手在手机周围，摄像头能捕捉到远场手部图像；Hand-Off，用户离开，摄像头捕捉不到手部图像。通过区分上述三种模态，智能手机能够更好地理解用户的非接触时行为，即长时间未触碰屏幕情况下的状态与意图，例如，用户是否在之后有交互需求，这有助于其进行更智能的反馈。

一个直接的应用是Hand-Around不锁定。智能手机认证与锁定作为一次自然的智能手机交互的起始步骤与结束步骤，其流程的合理性对安全性以及用户操作智能手机的效率与体验有着极其重要的影响。前人针对这个问题也进行了不同角度的研究。研究表明，智能手机用户平均花费约 2.9 \%的时间用于认证（最多的用户高达9\%），且用户认为在24.1\%的情况下安全性认证是不必要的\cite{185310}。基于上述事实，我们认为，更合理的智能手机认证与锁定流程能有效提升用户的使用效率与体验。现有智能手机的普遍锁定流程是：等待一段时间，若用户在该时间段内无与屏幕的交互，即电容屏感知不到触摸信号，则智能手机自动息屏并锁定。然而，这样的认证-锁定逻辑是不够智能的，因为用户在使用手机时，常常会有“非接触”且“不锁定”的需求。例如，用户需要以不确定的频率与手机进行频繁的交互，一个实际的场景是用户在与其他人通过社交软件进行多轮的聊天对话，此时，频繁的锁定-解锁过程是低效而繁琐的。我们针对上述情景，基于我们的手部全感知技术，提出了一种更高效的锁定-解锁流程：后台连续对当前手部状态进行检测，若为Hand-On或Hand-Around，则智能手机不主动锁定，当检测结果稳定为Hand-Off，则触发普通的解锁流程。

\begin{figure}[h]
  \centering
  \includegraphics[width=3in]{figures/method/HandAround.pdf}
  \caption{Hand-Around状态转移图}
  \label{fig:hand_around}
\end{figure}

为此，我们需要对单目图像进行Hand-On、Hand-Around、Hand-Off三个模态的分类，这同样可以通过训练以MobileNetV2为前馈特征提取网络的分类器\ref{cha:grip}来完成。

在实现上述分类的基础上，我们引入“待锁定”状态作为智能手机“使用中”与“锁定”两个状态的中间状态，并为Hand-Around不锁定设计了解锁-锁定状态转移流程图（如图\ref{fig:hand_around}）。



另一个创新性的应用是Hand-Around感应。智能手机可以感知用户手部的“非接触式”行为，例如手在空中摇晃、手接近手机、手准备拿起手机等，并从中理解用户的交互需求。例如当手机放在桌面，用户的手接近或准备拿起手机时，手机会自动亮起，在屏幕上显示出关键的信息，在用户拿起手机准备交互时，提前启动脸部识别完成验证——这极大地增加了用户日常交互流程的流畅性与效率。同样地，该应用也可以通过区分Hand-On、Hand-Around、Hand-Off三种模态来实现。


