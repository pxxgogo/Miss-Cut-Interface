\chapter{基于单目图像的智能手机手部全感知算法}
\label{cha:method}

在这一章中，我们将会给出问题的定义，详细地说明我们提出的RGB/IR双模态下基于单目图像的手部感知算法中每一步骤的原理，并就每一部分的实现细节进行讨论。最后，我们会对算法的每一部分从效率与准确率两个角度进行评测。

\section{问题定义}
由上一章所述，借助良好设计的硬件——通过3D打印外壳连接固定的三棱反射镜，我们可以通过智能手机前置摄像头实时捕获用户操作手机时的手部图像。我们以水平视角的经过三棱镜反射的手部RGB或者IR图像构成的视频流作为输入，希望通过计算机视觉算法，从图像中检测出与交互相关的手部信息，例如持握手势、点击手指，指尖位置等。基于这些额外的信息，再进一步进行交互设计。


\section{算法流水线}

我们的算法流水线如图\ref{fig:algorithm}，算法的输入为摄像头捕捉的视频流构成的单目RGB/IR图像序列，输出为握姿信息、指尖位置信息等通用特征以及下降模式等针对特定应用的特征。

首先，我们使用MobileNetV2模型对视频流中的视频帧进行握姿分类，实时地检测用户当前的握姿；在RGB单目图像下，我们使用MobileNetV2-SSD模型进行指尖区域的检测；在IR单目图像下，我们提出了一种基于握姿与边缘曲率统计特征的轮廓分类与指尖识别方法；最后，我们通过运动模式匹配与检测算法，检测指尖某种模态的运动过程（如下落、上升）。

与HandSee相比，我们的算法更具有通用性与高效性。其一，对于HandSee的完整算法流程，一个观察是，HandSee中预处理（13ms）、双目视觉匹配（10ms）、CNN手部分割（4ms）三个部分占总流程时长的80\%以上，且在服务器端计算需要花费27ms时间，无法满足移动端实时计算的需求；其二，HandSee的算法流程直接的输出是手部深度图的重构信息与通过简单形态学检测找出的指尖信息，而我们的算法流程可以得到包括握姿、优化的指尖、下降模式等更高级的语义信息。这侧面体现出我们的算法流程的科学性与合理性。

\begin{figure}
\centering
\includegraphics[width=5in]{figures/method/algorithm.pdf}
\caption{基于单目RGB/IR的全手行为感知算法流程图}
\label{fig:algorithm}
\end{figure}

\section{握姿检测模型}
\label{cha:grip}

\subsection{卷积神经网络}
卷积神经网络是图像分类任务中最普遍应用的神经网络模型。最基本的卷积神经网络模型由卷积层、池化层和全连接层构成，以下简单说明其工作原理。

\textbf{卷积层：}

首先定义二维卷积操作，对于$W \in R_{h,w}$，其每一个像素记作$W_{i,j}$；$s \ in R_{2k+1,2k+1}$，其每一像素记作$s_{i,j}$，定义$W$与$s$的卷积为$O = W \odot s$，则有：

\begin{equation}
    O_{m,n} = \sum\limits_{i = -k}^k \sum\limits_{j = -k}^k W_{m+i,n+j} \cdot s_{2k+1-i, 2k+1-j}
\end{equation}

其中，超出下标范围的$W_{i,j}$可根据不同的补全策略进行补全（例如，可以以全0进行填充）。

有了以上的卷积操作定义，我们可以进一步定义多输入、多输出通道的卷积层，设输入图像矩阵$I \in R_{h \times w \times d_i}$，其中h为图像矩阵的高度，w为图像矩阵的宽度，$d_i$为图像矩阵的输入（色彩）通道数，它的每一个像素记作$I_{i,j,k}$，卷积核$s \in R_{d_i \times k \times k \times d_o}$，每一位置权重记作$s_{i,j,k,l}$，输出的矩阵为$O \in R_{h \times w \times d_o}$，则有

\begin{equation}
    O_{m,n,p} = \sum\limits_{c = 0}^{d_i - 1} \sum\limits_{i = -k}^k \sum\limits_{j = -k}^k I_{i,j,c} \cdot s_{c, 2k+1-i, 2k+1-j, p}
\end{equation}

进一步，可定义不同步长的卷积层，设步长为l，则有：
\begin{equation}
    O_{m,n,p} = \sum\limits_{c = 0}^{d_i - 1} \sum\limits_{i = -k}^k \sum\limits_{j = -k}^k I_{m \cdot l+i,n \cdot l+j,c} \cdot s_{c, 2k+1-i, 2k+1-j, p}
\end{equation}


\textbf{池化层：}

池化层的基本结构如下。设输入图像矩阵为$I$，池化核大小为k，输出为$O$，则有：
\begin{equation}
    O_{m,n,p} =  {1 \over k^2} \sum\limits_{i = -k}^k \sum\limits_{j = -k}^k I_{m \cdot l+i,n \cdot l+j,p}
\end{equation}

有了以上网络结构，对于图像矩阵为$I$，可通过卷积层与池化层的组合对其进行特征提取，并将其最终输出的特征通过全连接网络输出最终各类别对应的分数为$x=(x_1, x_2, \cdots, x_n)$，设该图片对应的标注类别为$class \in \{1,2,\cdots,n\}$，则可用交叉熵损失函数来刻画x的损失：

\begin{equation}
    CrossEntropyLoss(x,class) = -log{exp(x_{class}) \over \sum\limits_{i=1}^n exp(x_i)}
\end{equation}

通过求导的链式法则可计算损失函数$L$对于网络中每一个参数$\theta_i$的导数$\partial L \over \partial i$，并可通过不同参数的更新策略（以最简单的随机梯度下降为例）对参数进行更新（$\alpha$为学习率），实现图像特征与分类的学习：

\begin{equation}
    \theta_{t+1} = \theta_t - \alpha {\cdot \partial L \over \partial \theta}
\end{equation}

理论与实验表明，单个卷积核能很好地捕捉图像的局部特征，通过卷积核在同一层内与不同层间的堆叠，可以实现多局部特征、高层语义特征的学习。

\subsection{MobileNetV2模型结构}

MobileNetV2为卷积神经网络中的一种网络结构设计，它将ResNet的残差层与MobilNet中深度可分离卷积的思想相结合，兼有二者的准确性与计算高效性特点。

由于MobileNetV2计算效率的高效性，我们使用MobileNetV2 作为握姿检测的前馈特征提取网络。MobileNetV2的原理与网络结构如下。

\textbf{深度可分离卷积：}深度可分离卷积的核心思想是将传统卷积层分解成横向与纵向两个卷积层。第一层为一个单卷积层，它被应用与输入的每个维度，第二层为$1 \times 1$的多通道卷积层（也称为点卷积）。具体地，设输入$L_i \in R_{h_i \times w_i \times d_i}$，传统卷积核$K \in R_{k \times k \times d_i \times d_j}$，卷积操作后得到输出$L_j \in R_{h_j \times w_j \times d_j}$；而对于深度可分离卷积，第一层卷积核$K_1 \in R_{k \times k \times d_i}$，它与$L_i$卷积后得到$L_{middle} \in R_{h_j \times w_j \times d_i}$，第一层卷积核$K_2 \in R_{1 \times 1 \times d_i \times d_j}$，卷积操作后得到输出$L_j \in R_{h_j \times w_j \times d_j}$。

\textbf{线性瓶颈模块与反残差层：}理论表明\cite{Sandler_2018}，ReLu层会对模块的输出特征造成破坏，因此线性瓶颈模块在借鉴了传统残差模块的瓶颈模块（ResNet）的同时，改变了其特征压缩/扩张策略，并用深度可分离卷积替换其传统卷积层，形成反残差层。反残差层的网络结构如表\ref{tab:InverseResidual}。

\begin{table}[htbp]
  \centering
  \caption{反残差层的网络结构}
  \label{tab:InverseResidual}
    \begin{tabular}{p{80pt}p{120pt}p{80pt}}
      \toprule
      Input & Operator & Output\\
      \midrule
      $h \times w \times k$  &  $1 \times 1$ conv2d, ReLU6 &   $h \times w \times (tk)$ \\
      $h \times w \times (tk)$  &  $3 \times 3$ dwise s, ReLU6 & ${h \over s} \times {w \over s} \times (tk)$ \\
      ${h \over s} \times {w \over s} \times (tk)$ &   linear $1 \times 1$ conv2d & ${h \over s} \times {w \over s} \times k'$\\
      \bottomrule
    \end{tabular}\\[5pt]
\end{table}

其中，t表示扩张因子，其作用是从输入中扩展更多通道的特征；s表示depth-wise卷积操作的步长，当$s=1$时输出时加上残差，结构类似于ResNet中的残差层，当$s=2$功能类似于池化层。

进一步地，以上述反残差层为基本单元，MobileNetV2的网络结构如表\ref{tab:MobileNetV2}


\begin{table}[htbp]
  \centering
  \caption{MobileNetV2网络结构}
  \label{tab:MobileNetV2}
    \begin{tabular}{p{80pt}p{80pt}p{40pt}p{40pt}p{40pt}p{40pt}}
      \toprule
      Input & Operator & t & c & n & s\\
      \midrule
      $224^2 \times 3$  &  conv2d & - & 32 & 1 & 2 \\
      $112^2 \times 32$  &  bottleneck & 1 & 16 & 1 & 1 \\
      $112^2 \times 16$  &  bottleneck & 6 & 24 & 2 & 2 \\
      $56^2 \times 24$  &  bottleneck & 6 & 32 & 3 & 2 \\
      $28^2 \times 32$  &  bottleneck & 6 & 64 & 4 & 2 \\
      $14^2 \times 64$  &  bottleneck & 6 & 96 & 3 & 1 \\
      $14^2 \times 96$  &  bottleneck & 6 & 160 & 3 & 2 \\
      $7^2 \times 160$  &  bottleneck & 6 & 320 & 1 & 1 \\
      $7^2 \times 320$  &  conv2d $1\times1$ & - & 1280 & 1 & 1 \\
      $7^2 \times 1280$  &  avgpool $7\times7$ & - & - & 1 & - \\
      $1^2 \times 1280$  &  conv2d $1\times1$ & - & k & - & - \\
      \bottomrule
    \end{tabular}\\[5pt]
\end{table}


具体地，对于握姿检测任务，设待区分的握姿集合为$G=\{G_1,G_2,...,G_m\}$，对于单独的视频帧$f_k$，设通过MobileNetV2提取的图像特征向量为$v_k=MobileNet(f_k) \in R_{n}$，设$W \in R_{n \times m}, b \in R_{m}$，则$f_k$对于$G$的概率向量为$p = Softmax(v_i \cdot W + b)$，其中$Softmax(x)_i = {{exp(x_i)} \over {\sum_i exp(x_i)}}$。设q为$f_k$的one-hot事实标注，则交叉熵损失函数可表示为$H(q,p)=-\sum_i q_i logp_i$。

除此之外，还可以以相邻的k帧作为输入进行预测。设对于相邻的k帧$f_1,f_2,\cdots,f_k$，它们的特征向量$v_i=MobileNet(f_i),i=1,2,\cdots,k$，将它们拼接后得到长度为nk的特征向量$v=[v_1;v_2;\cdots;v_{nk}]$，类似地可得该视频帧序列对于$G$的概率向量为$p = Softmax(v \cdot W + b)$。

\subsection{参数优化与训练策略}

我们使用Adam优化器\cite{2014arXiv1412.6980K}对网络参数进行优化，它的参数更新规则如公式\ref{adam}。

\begin{equation}
\label{adam}
\left\{\begin{array}{l}
    g_t \leftarrow \nabla_\theta H_t(\theta_{t-1})\\[0.3cm]
    m_t \leftarrow \beta_1 \cdot m_{t-1} + (1-\beta_1) \cdot g_t\\[0.3cm]
    v_t \leftarrow \beta_2 \cdot v_{t-1} + (1-\beta_2) \cdot g_t^2\\[0.3cm]
    \hat{m_t} \leftarrow {m_t \over {1- \beta_1^t}}\\[0.3cm]
    \hat{v_t} \leftarrow {v_t \over {1- \beta_2^t}}\\[0.3cm]
    \theta_t \leftarrow \theta_{t-1} - {\alpha \cdot {\hat{m_t}} \over {(\sqrt{\hat{v_t}} + \epsilon)}}
\end{array}\right.
\end{equation}

Adam优化器结合了AdaGrad与RMSProp两种优化算法，综合考虑梯度的一阶矩与二阶矩估计，其具有计算效率高、参数更新不受梯度伸缩变换的影响、解释性好等优点，适合用作我们握姿分类网络训练的优化器。




\section{指尖识别算法}
\label{cha:fingertip}
单目RGB图像与单目IR图像特征差异较大，前者往往需要处理诸如背景干扰、光照条件的一致性等问题，而后者往往可以得到清晰、纯净的手部图像。因此，我们对于单RGB图像与单IR图像，分别提出了两种不同的指尖检测算法策略。对于前者，我们使用基于MobileNetV2的Single Shot Detection（
SSD）神经网络模型进行检测；对于后者，我们设计了一种基于基于形态学特征的指尖检测算法。

\subsection{基于单RGB图像的指尖检测模型}

对于单RGB图像，我们使用以MobileNetV2作为前馈特征提取网络的SSD（Single Shot Detection）神经网络模型直接对指尖区域进行提取。我们按照\inlinecite{Sandler_2018}与\inlinecite{Huang2017Speed}的设定进行网络的构建。首先将传统的SSD网络中所有的卷积层替换成深度可分离卷积，我们将替换过后的SSD网络称为SSDLite。对于MobileNetV2，我们将MobileNetV2第15层的输出（输出的步长为16）作为SSDLite第一层的输入，将MobileNetV2最后一层的输出（输出步长为32）接至SSDLite第二层及之后。


对于multibox-classifier，其损失函数可表示为：

\begin{equation}
    L(x,c,l,g)={1 \over N}(L_{conf}(x,c) + \alpha L_{loc}(x,l,g))
\end{equation}

其中$L_{loc}$为预测框的位置损失函数，$L_{conf}$为置信度损失函数：

\begin{equation}
    L_{loc}(x,l,g)= \sum_{i \in Pos}^N \sum_{m \in \{cx, cy, w, h\}} x_{ij}^k smooth_{L1}(l_i^m-\hat{g}_j^m)
\end{equation}

\begin{equation}
    L_{conf}(x,c)=- \sum_{i \in Pos}^N x_{ij}^p log(\hat{c}_i^p) - \sum_{i \in Neg} log(\hat{c}_i^0), \hat{c}_i^p = {exp(c_i^p) \over {\sum_p exp(c_i^p)}}
\end{equation}

其中$x_{ij}^p \in {0,1}$表示第i个候选框与第j个事实框关于第p类是否匹配（IOU>0.5）；c表示某个候选框预测的分类置信度，其中第0类表示背景；l为预测框；g为事实框。

\subsection{基于单IR图像的指尖识别算法}
对于单IR图像，由于其背景纯净，直接通过阈值分割的方法提取手部区域是十分有效的，因此在这一节，我们使用基于形态学特征的指尖检测算法对指尖进行检测。与\inlinecite{Chen:2014:AIT:2642918.2647392}和\inlinecite{Yu:2019:HEF:3290605.3300935}单纯依赖区域轮廓的简单几何特征（例如到中心点的距离、局部曲率）进行指尖预测不同，我们对指尖检测算法进行了基于握姿、轮廓形态、轮廓大小、轮廓位置的更细致的设计。

\subsubsection{边缘检测与轮廓检测}

我们使用Sobel算子对图像的边缘进行检测，对于图像A，  分别使用横向、纵向的Sobel算子对图像进行滤波，分别得到A的横向边缘$G_x$与纵向边缘$G_y$：

\begin{equation}
    G_x = \left[
    \begin{matrix}
        -1 & 0 & +1 \\
        -2 & 0 & +2 \\
        -1 & 0 & +1
    \end{matrix}
    \right] \cdot A,
    G_y = \left[
    \begin{matrix}
        +1 & +2 & +1 \\
        0 & 0 & 0 \\
        -1 & -2 & -1
    \end{matrix}
    \right] \cdot A
\end{equation}

我们将横向边缘$G_x$与纵向边缘$G_y$结合，得到A的边缘特征$G$：
$$G=\sqrt{G_x^2 + G_y^2}$$

随后我们通过一种基于拓扑结构分析的二值图像边界跟踪算法\cite{Suzuki1984}对以上边缘图像进行连通区域的提取，进而可得到图像中所有连通区域的轮廓所构成的集合。

\subsection{基于边缘曲率特征的区域特征检测}

对于视频帧f，首先通过边缘检测提取出图像边缘，紧接着通过轮廓检测算法提取每一个连通区域的轮廓，得到轮廓集合$Edges_f = \{e_i = edge(R_i), i=1,2,\cdots,n\}$，其中$R_i$为第i个连通区域。我们首先根据轮廓的形态、大小、位置以及握姿的信息将每个轮廓划分为以下类别之一：左持握手拇指、右持握手拇指、双手拇指、持握手手指（非拇指）、操作手、其他（可能是非手部区域）。

对于手部形态特征，我们采用了一种创新的基于边缘曲率分布的区域特征度量方法。对于曲线C上的点p，C在p处的曲率为$lim_{\Delta s \to 0} {{\Delta \alpha}\over{\Delta s}}$，可将其转化为等价的离散形式：

\begin{equation}
    curve(p,k)=lim_{\Delta s \to 0} {{\Delta \alpha}\over{\Delta s}} = {|y''| \over {(1+y'^2)^{1.5}}} = {2h \over{\sqrt{h^2+d^2}\cdot 2d}}
\end{equation}

其中，h为$p_i$到直线$p_{i-k}p_{i+k}$的有向距离，2d为$||p_{i-k}-p_{i+k}||$，即$p_{i-k}$到$p_{i+k}$的距离。由于选取的离散点为相邻点或等间隔点，这里的曲率特征也可近似替换为偏转角度特征$\hat{curve}(p)={h \over 2d}$。我们通过曲率特征不同尺度的堆叠来刻画轮廓在某点处及其周围的特征：$feat(p_i) = \{curve(p_i,k), k \in K\}$，其中K为我们感兴趣的尺度集合，一般地，可取$K=\{1, 2, 4, 8, 16, 32, 64\}$。对于某个轮廓的形态特征$F_e$，可以将其表示为其中所有点的曲率特征的集合：$F_e = \{feat(p), p \in e\}$。由于轮廓的大小与尺度的不同，需要对特征在空间维度与曲率大小维度进行归一化。归一化后，所有的轮廓的特征均可表示为一个大小相等的矩阵。由于特征具有空间上的轮换不变性，两个特征矩阵的距离可表示为
\begin{equation}
    dist(M_{e1}, M_{e2}) = min\{||M_{e1}[l+1,\cdots,n,1,\cdots,l]-M_{e2}||_F, l = 1,\cdots,n\}
\end{equation}
其中$M_{e1}[l+1,\cdots,n,1,\cdots,l]$表示$M_{e1}$在空间维度上的一个轮换排列。

\subsection{基于特征融合的区域匹配与指尖匹配算法}

通过上一小节的边缘曲率特征的区域特征检测算法，我们可以得到图像中轮廓的特征表示。对于握姿信息，可以通过上节的MobileNetV2网络分类获得；对于面积、位置信息，直接计算轮廓所包围区域的面积、质心；对于边缘位置信息，直接统计轮廓在图像边缘上的点的数目。随后，可以通过人工设计的经验规则及形态特征匹配的数据对区域进行分类：

\begin{equation}
    p(e) = 0.5\times p_{rules} + 0.5 \times p_{shape}=0.5 \times M_1 \cdot feature(e) + 0.5 \times KNN(e, G)
\end{equation}

其中$M_1$为权值矩阵，$feature(e)$为特征向量，$G$为有标注的小样本数据集，$KNN(e, G)$为与e最接近的k个样本投票的预测值。

基于轮廓分类与手部形态特征，可进行细粒度的指尖检测。对于轮廓e，设$P=\{p_1, p_2, \cdots, p_t\}$为e中的曲率极值点集，对于极值点p，对以p为中心大小为d的窗内的分布在已知的指尖模板集合G中相同轮廓类别的子集G'中进行匹配，若二者距离小于阈值，则将p加入指尖候选集。

\section{运动模式的匹配与检测}
\label{cha:motion}
设$S_k = \{p_{k,1}, p_{k,2}, ... , p_{k,m_k}\}$为第k个视频帧所对应的指尖候选点集，则我们需要检测的是视频流所对应的候选点集序列$S_0, S_1, ... , S_k$中是否存在满足下降运动模式$P$的点的序列。我们首先来定义下降运动模式$P$，我们称点序列$s_p=\{p_{a_i,b_i}|i=1,2,...,m; a_1 < a_2 < ... < a_m\}$满足运动模式$P$当且仅当$$\forall i \in \{1,2,...,m-1\}, 0 < y_{p_{i+1}} - y_{p_i} < \epsilon_1, |x_{p_{i+1}} - x_{p_i}| < \epsilon_2$$

其中$y_{p_{i+1}} - y_{p_i} > 0$保证了下降趋势，$y_{p_{i+1}} - y_{p_i} < \epsilon_1$ 和 $|x_{p_{i+1}} - x_{p_i}| < \epsilon_2$保证了候选点的一致性。

进一步地，可以使用动态规划的策略对问题进行求解：

\begin{algorithm}
\caption{使用动态规划求解最长下降子序列长度}
\begin{algorithmic}
\Require 相邻的k帧的指尖候选点集$S_1, S_2, \cdots, S_k$，其中$S_i = \{p_{i,1}, p_{i,2}, ... , p_{i,m_i}\}$
\Ensure 最长子序列长度

定义函数$f: \bigcup_i S_i \rightarrow N^*-\{0\} $
\For{i = 1, 2, $\cdots$, k}

    \For {$p \in S_i$}
    
        \State $f(p) \leftarrow 1$
        
        \For {$q \in \{p_{u,v}, u < i\}$}
        
            \If {$0 < y_{p} - y_{q} < \epsilon_1, |x_{p} - x_{q}| < \epsilon_2$}
            
                \State $f(p) \leftarrow max(f(p), f(q)+1)$;
                
            \EndIf
        
        \EndFor
        
    \EndFor
    
\EndFor

最长子序列长度 = $max(\{f(p), p \in \bigcup_i S_i\})$

\end{algorithmic}
\end{algorithm}

对于相邻的k帧的指尖候选点集$S_k = \{p_{k,1}, p_{k,2}, ... , p_{k,m_k}\}$，设其满足下降运动模式$P$的最长子序列$s_p$长度为$l={max(L(s_p)) \over k} $，则定义置信系数$\lambda={l \over k}={max(L(s_p)) \over k}$。当$\lambda > \lambda_0$，可认为在该帧区间内发生指尖下降行为。

由于视频帧有一定概率出现模糊，导致指尖候选集的结果错误或不完整，因此需要一定的容错机制，这在算法中通过不强制限定帧间连续来实现。

\section{算法评测}

在本节，我们构建了RGB与IR的握姿分类与指尖检测数据集，并在此数据集的基础上对我们提出的RGB/IR双模态下基于单目图像的手部感知算法的每一步骤的效率与准确率进行评估。

\subsection{数据集}
\label{dataset}

\textbf{RGB数据集：}我们收集了10位实验者的7种不同握姿的RGB图像数据，7种不同握姿包括：左手单手操作、右手单手操作、左手持握右手操作、右手持握左手操作、双手拇指操作、左手横向持握、右手横向持握。每位实验者要求使用每种握姿操作手机，并录制一分钟的视频，视频分辨率为$640 \times 480$，帧率为30fps，每种握姿共产生$10 \times 60 \times 30=18000$视频帧，总共可获得126000帧带握姿分类标签的图像数据。对于握姿分类任务，我们将数据集按照8名实验者/1名实验者/1名实验者的方式划分训练、验证与测试数据集。对于指尖检测任务，我们随机从前五种握姿中抽取1000帧图像，对这1000帧图像进行人工的指尖标注，每张图中标注出所有的指尖候选框，并为候选框进行四分类（持握手拇指、持握手其他手指、操作手拇指、操作手其他手指）。标注数据以COCO数据集格式存储。

\textbf{IR数据集：}我们收集了2位实验者的5种不同握姿的RGB图像数据，5种不同握姿包括：左手单手操作、右手单手操作、左手持握右手操作、右手持握左手操作、双手拇指操作。实验者在完全黑暗的环境下使用每种握姿操作手机，并录制一分钟的视频，视频分辨率为$640 \times 480$，帧率为30fps，每种握姿共产生$2 \times 60 \times 30=3600$视频帧，总共可获得7600帧带握姿分类标签的图像数据。对于握姿分类任务，我们将数据集按照1名实验者/1名实验者的方式划分训练与测试数据集。对于指尖检测任务，我们随机抽取100帧图像，对这100帧图像进行轮廓标注（左持握手拇指、右持握手拇指、双手拇指、持握手四指、操作手、其他）与指尖标注（在轮廓上）作为轮廓模板与指尖模板。

\subsection{算法效率评测}

首先，为了测试我们的算法流水线的效率，我们在真实的硬件条件下（Galaxy S9+，2.6GHz单线程）测试了握姿分类、RGB指尖检测、IR指尖检测、指尖下降检测等步骤的用时。

在真实的硬件条件下（Galaxy S9+，CPU 2.8GHz，8核心，ARM架构），算法流程中每个步骤的用时如表\ref{tbl:alg_speed}所示（单位：毫秒）。

\begin{table}[htbp]
\centering
\caption{算法流水线效率}
\label{tbl:alg_speed}
\begin{tabular}{p{40 pt}p{50 pt}p{70 pt}p{70 pt}p{70 pt}p{40 pt}}
 \toprule
  & 握姿分类 & RGB指尖识别 & IR指尖识别 & 下降模式检测 & 总用时\\
 \midrule
单线程 & 66.1 & 35.1 & 13.3 & 2.5 & 103.7 \\
多线程 & 16.5 & 11.0 & 13.3 & 2.5 & 32.3 \\
\bottomrule
 \end{tabular}\\[2pt]
\end{table}

由以上数据可知，我们的算法在移动设备上在多线程计算的条件下可达到30fps的帧率，这说明我们的算法流程能够满足在移动设备上实时计算的需求。我们对比了我们的流程与HandSee的流程的计算量，可以发现，HandSee的计算量主要集中在预处理（13ms）、双目视觉匹配（10ms）、CNN手部分割（4ms）三个步骤，仅这三个步骤在服务器端计算已需花费27ms的时间。除此之外，HandSee的算法流程直接得到的是手部深度图的重构信息与通过简单形态学检测找出的指尖信息，而我们的算法流程可以得到包括握姿、优化的指尖、下降模式等更高级的语义信息。因此，我们的算法流程无论是从效率还是科学性上看，均有较好的表现。

为了说明我们在模型选择时的一些权衡，体现我们的方法的效率优势，我们对比了四种经典神经网络模型MobileNetV2、DenseNet161、ResNet152、SqueezeNet 在握姿7分类的任务上分别运算于Nvidia GTX 1080Ti与2.6Ghz单线程Intel x86 CPU的效率，比较结果如表\ref{tbl:nn_speed}所示（单位为毫秒）。由以上结果，我们可以看出，DenseNet与ResNet作为高准确率模型的代表，其模型结构复杂、参数量与计算量大，在CPU下几乎无法支持实时计算；SqueezeNet1.1虽然计算效率极高，但是性能相对较差，不太适合作为SSD的前馈网络；MobileNetV2在兼顾性能与准确率下表现较好。

\begin{table}[htbp]
\centering
\caption{不同神经网络模型效率比较}
\label{tbl:nn_speed}
\begin{tabular}{p{80 pt}p{100 pt}p{100 pt}}
 \toprule
   & Nvidia GTX 1080Ti & x86 CPU @ 2.6Ghz\\
 \midrule
    DenseNet161 & 27.9 & 139.1 \\
    ResNet152 & 23.9 & 369.8 \\
    SqueezeNet1.1 & 2.6 & 6.4 \\
    MobileNetV2 & 5.8 & 54.3 \\
\bottomrule
 \end{tabular}\\[2pt]
\end{table}

\subsection{握姿分类准确率评测}

对于握姿分类，我们在带类别标注的RGB数据集上（5类与7类），分别使用在ImageNet上预训练的MobileNetV2网络进行finetune训练，使用Tensorboard记录训练步骤的损失与每个epoch验证集的准确率。除此之外，我们还使用了DenseNet161、ResNet152和SqueezeNet1.1进行了相同的测试，并横向比较了不同网络结构在检测性能上的差异。

我们分别在5分类RGB数据集、7分类RGB数据集、5分类IR数据集上测试了MobileNetV2、DenseNet161、ResNet152和SqueezeNet1.1四种模型握姿分类的准确率，准确率如表\ref{tbl:ges_class}。

\begin{table}[htbp]
\centering
\caption{握姿分类正确率}
\label{tbl:ges_class}
\begin{tabular}{p{80 pt}p{100 pt}p{100 pt}p{100 pt}}
 \toprule
   & IR @ 5 classes & RGB @ 5 classes & RGB @ 7 classes\\
 \midrule
    DenseNet161 & 99.65\% & 98.88\% & 99.19\% \\
    ResNet152 & 97.23\% & 98.31\% & 96.77\% \\
    SqueezeNet1.1 & 95.96\% & 94.94\% & 95.56\% \\
    \textbf{MobileNetV2} & \textbf{96.89\%} & \textbf{96.63\%} & \textbf{97.98\%} \\
\bottomrule
 \end{tabular}\\[2pt]
\end{table}

由以上结果可知，MobileNetV2模型在RGB图像5分类、RGB图像7分类、IR图像5分类三个任务上都有较好的表现，准确率均大于96.5\%。虽然与DenseNet161仍有一定的差距，但是综合考虑模型的计算量与精度等因素，该模型保证了计算量能够满足移动实时计算的同时，也保持了较高的精度，同时保证了后续SSD模块的有效性，是一个较为合理的选择。

\subsection{RGB指尖识别准确率评测}

对于RGB指尖识别，我们使用上述带指尖标注的COCO格式RGB图像数据训练MobileNetV2-SSD网络，使用Tensorboard记录训练步骤的分类损失、位置损失、正则损失及困难样本数目，以及每个epoch验证集上的平均准确率均值（mAP）与平均召回率（AR），并对实验结果进行分析与讨论。

我们在此给出每个epoch验证集上的平均准确率均值（mAP）与平均召回率（AR）。对于平均准确率均值，我们感兴趣的指标包括mAP、mAP@0.5IOU、mAP@0.75IOU、mAP small、mAP medium和mAP large。其中mAP、mAP@0.5IOU、mAP@0.75IOU分别指在1、0.5、0.75IOU（IOU指对于两个区域，它们的交区域与它们的并区域的比值）下的平均准确率均值；mAP small、mAP medium和mAP large指小、中、大候选区域下的平均准确率均值。具体结果如表\ref{tbl:fingertip_precision}。

\begin{table}[htbp]
\centering
\caption{平均准确率均值}
\label{tbl:fingertip_precision}
\begin{tabular}{p{60 pt}p{60 pt}p{60 pt}p{60 pt}p{60 pt}p{60 pt}}
 \toprule
  mAP & mAP@0.5 & mAP@0.75 & mAP@S & mAP@M & mAP@L\\
 \midrule
    0.73 & 0.98 & 0.90 & 0.63 & 0.77 & 0.85 \\
\bottomrule
 \end{tabular}\\[2pt]
\end{table}

对于平均准确率均值，我们感兴趣的指标包括AR@1、AR@10、AR@100、AR@100S、AR@100M和AR@100L。其中AR@1、AR@10、AR@100分别指置信度最高的前1、10、100个候选框的平均召回率；AR@100S、AR@100M、AR@100L分别指小、中、大候选区域下的平均召回率。具体结果如表\ref{tbl:fingertip_recall}。

\begin{table}[htbp]
\centering
\caption{平均召回率}
\label{tbl:fingertip_recall}
\begin{tabular}{p{60 pt}p{60 pt}p{60 pt}p{60 pt}p{60 pt}p{60 pt}}
 \toprule
  AR@1 & AR@10 & AR@100 & AR@100S & AR@100M & AR@100L\\
 \midrule
    0.56 & 0.78 & 0.78 & 0.70 & 0.83 & 0.88 \\
\bottomrule
 \end{tabular}\\[2pt]
\end{table}

由以上结果可知，在0.75IOU的情况下，平均准确率均值可达0.9，即检出的候选框在0.75IOU下的正确率可达到90\%；置信度前10的候选框可以召回图像中78\%的指尖区域。一个观察是：对于较小的区域（例如持握手四指区域），准确率与召回率均低于中型与大型区域（例如持握手拇指、操作手手指）。这可能是由于持握手四指区域不完整而且没有显著的区分性特征，较容易混淆。对于主要的手指区域（持握手拇指、操作手手指），上述的结果表明我们的模型在这些区域上的可用性较好。一个输出示例如图\ref{fig:rgb_example}所示。

\begin{figure}[h]
  \centering
  \includegraphics[width=5in]{figures/appendix/individualImage.png}
  \caption{RGB指尖检测结果示例}
  \label{fig:rgb_example}
\end{figure}

\subsection{IR指尖识别准确率评测}

对于IR指尖识别，我们在IR数据集上运行我们的指尖检测算法，在生成的实验结果中随机抽取100帧进行人工标注，标注的内容包括实际的指尖数目、检出的指尖数目、正确检出的指尖数目。标注结果如表\ref{tbl:fingertip_ir_result}。


\begin{table}[htbp]
\centering
\caption{IR指尖检测准确率}
\label{tbl:fingertip_ir_result}
\begin{tabular}{p{60 pt}p{60 pt}p{60 pt}p{60 pt}p{60 pt}p{60 pt}}
 \toprule
   指尖数 & 检出指尖数 & 正确指尖数 & 准确率 & 召回率 & F1 \\
 \midrule
    249 & 231 & 217 & 93.94\% & 87.15\% & 0.904 \\
\bottomrule
 \end{tabular}\\[2pt]
\end{table}


\begin{figure}[h]
  \centering
  \includegraphics[width=6in]{figures/experiment/fingertip_hist.png}
  \caption{基于曲率直方图特征指尖检测的匹配示例}
  \label{fig:fingertip_hist}
\end{figure}

由以上结果可知，对于IR图像，我们的指尖检测算法准确率可达93.94\%，表现较好。我们观察了检测失败的一些负例，发现检测失败的图像普遍具有以下特点（之一）：由于阴影、遮挡或运动模糊等问题造成的手部轮廓检出不完整；手部轮廓在透视视角下发生遮挡或粘连；指尖位于轮廓区域内部，且轮廓无明显特征。我们希望在后续工作中，尝试解决上述三个问题。但整体而言，单目IR图像下的多指尖检测效果较好，检测质量普遍优于RGB指尖检测。

我们观察了基于曲率分布特征的指尖检测的中间结果，如图\ref{fig:fingertip_hist}所示，左下角为原始图像与检出的轮廓，全图为曲率特征在不同采样频率下的分布。可以看出，在曲率特征度量下，指尖点、指缝点以及全手的形态特征均能得到较好的表示。这也进一步论证了我们的方法的有效性。一个输出示例如图\ref{fig:ir_example}所示，其中，输出的信息包括，每个区域的分类（左、右持握手拇指、持握手四指、操作手、Hand-To-Hand）与区域的指尖。

\begin{figure}
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[height=2in]{figures/experiment/class_1.png}
\end{minipage}%
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[height=2in]{figures/experiment/class_2.png}

\end{minipage}
\caption{IR图像指尖检测结果示例}
\label{fig:ir_example}
\end{figure}

