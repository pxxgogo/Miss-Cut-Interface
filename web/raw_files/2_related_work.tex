\chapter{文献综述}
\label{cha:related_work}

在本章中，我们首先回顾了前人在智能手机手部交互增强的可能性上进行的探索，并就这些工作中的交互空间、交互信息与传感设备进行细致的讨论。然后，我们对目前普遍应用的手部传感技术进行概述，并重点介绍基于视觉的手部传感技术。最后，我们会从技术上对图像分类、物体检测等领域的工作进行总结。

\section{智能手机手部交互增强}

为了克服手机触摸屏的局限性，许多前人的研究工作对智能手机手部感知增强与交互方式的可能性进行了探索，例如延拓交互空间（背面\cite{Corsten:2017:BUB:3025453.3025565}\cite{Wong:2016:BBO:2999508.2999522}或侧面\cite{Chang2006Recognition}\cite{Cheng:2013:IAS:2470654.2481424}\cite{Cheng:2013:IGA:2468356.2479514}或在设备\cite{Chen:2014:AIT:2642918.2647392}\cite{Hasan:2016:TST:2983310.2985755}\cite{Hinckley:2016:PSM:2858036.2858095}上方）、扩展触摸模态或输入模态（例如，模态识别\cite{Harrison:2011:TEF:2047196.2047279}\cite{Xiao:2015:EFA:2817721.2817737}与手指识别\cite{Masson:2017:WIF:3126594.3126619}）、引入持握信息（例如，界面移动\cite{Cheng:2013:IAS:2470654.2481424}\cite{Lim:2016:WAR:2957265.2961857}）等。这些工作从交互空间上看，可分为触摸屏幕、触屏上方的空间、触屏背面的空间、设备周围；从交互信息上看，扩展的信息包括握姿、点击手指、手指点击部位、手指点击角度、手指空间信息等；从感测硬件设备的类型上看，包括电容传感器、陀螺仪、加速度传感器、空间位置传感器（如OptiTrack）、声学传感器、RGB摄像头、IR摄像头（红外摄像头）等。

从交互空间上看，扩展的交互空间拓展了交互的可能性，智能手机背面、侧面、上方、周围等空间上的交互在针对不同任务时体现出其独特的优越性。BackXPress\cite{Corsten:2017:BUB:3025453.3025565}使用户在双手持握双手拇指操作的握姿下能通过背后四指的压力来控制拇指的输入模态；iRotateGrasp\cite{Cheng:2013:IAS:2470654.2481424}和iGrasp\cite{Cheng:2013:IGA:2468356.2479514}通过屏幕侧边的电容传感器感知握姿，并自适应调整屏幕朝向与键盘布局；Air+Touch\cite{Chen:2014:AIT:2642918.2647392}通过外置的深度摄像头捕获空中的手部信息，将高度互补的触摸与空中手势相结合（触摸用于指定目标与切分空中手势，而空中手势则增加了触摸事件的表现力），形成一种新的融合的交互模式。

从交互信息上看，更强的手机传感功能可以带来额外的交互相关的手部信息，例如握姿、点击手指、手指点击部位、手指点击角度、手指空间信息等。TapSense\cite{Harrison:2011:TEF:2047196.2047279}将点击瞬间的声音信息与电容信号相结合，区分不同手指部位（如指尖、指甲、指关节、手掌等）的点击；\inlinecite{Xiao:2015:EFA:2817721.2817737}通过屏幕电容信号的形状与强度估计点击手指的3D倾角；Pre-touch\cite{Hinckley:2016:PSM:2858036.2858095}通过增强电容屏的灵敏度与感测范围，实现对设备周围手部与手指的感知；HoverFlow\cite{Kratz:2009:HED:1613858.1613864}使用红外距离传感器捕获设备附近的手，以感知用户的手部运动和静态手部姿势。

上述的交互设计根据交互信息需求，配备了不同类型的传感器，包括电容传感器、陀螺仪、加速度传感器、空间位置传感器（如OptiTrack）、声学传感器、RGB摄像头、IR摄像头（红外摄像头）等。前人通过直接处理、间接处理、多模态融合等方式，从传感器采集的原始数据中挖掘出交互相关的信息。

从交互应用的角度上看，上述所有的交互技术均是面向应用，以应用为导向的——只有配备经过良好设计的交互应用，才能发挥交互技术的优势。例如，对于持握感知，不同研究使用不同传感器采集握姿原始数据，然后通过SVM或卷积神经网络等方法对握姿进行分类，输出分类结果，并将其应用至后续交互应用中。这些交互应用包括页面自动旋转、界面布局调整、应用切换预测、键盘布局微调、自动触发应用等。持握感知技术所感知的交互信息是有限制的，但是将其作为后续交互应用的输入，却带给人们无限的可能——这些交互应用通过合理地处理握姿信息，给用户提供了更高效、更智能、更愉悦的用户体验。


\section{手部识别追踪技术}

准确的手部跟踪对于人机交互的研究具有重要的意义。前人已经研究了在使用手机这个场景下的各种跟踪技术，例如使用电容传感器\cite{le2018infinitouch}、红外信号\cite{han2005low}、超声波\cite{Nanayakkara:2013:EFI:2459236.2459240}、毫米波雷达\cite{Lien:2016:SUG:2897824.2925953}和单目RGB摄像头\cite{Mullender:1993:DS(:302430}或深度摄像头\cite{Sharp:2015:ARF:2702123.2702179}或两者的组合。在这些技术中，其中一些只能检测动态手势（例如雷达和超声波），其他技术都可以同时捕获动态手势和静态姿势。

在这些技术中，基于摄像头的手部跟踪技术显示出了提供完整场景信息的最大潜力。在前人文献中，有大量的工作将相机部署在设备\cite{Chen:2014:AIT:2642918.2647392}\cite{Sridhar:2017:WAI:3025453.3026005}上，人体\cite{Chan:2015:CEW:2807442.2807450}\cite{Kim:2012:DFI:2380116.2380139}\cite{Mistry:2009:SWG:1667146.1667160}上，以及在环境\cite{Sharp:2015:ARF:2702123.2702179}中以完成手部跟踪。在商业上，基于深度摄像头的LeapMotion在大桌面和VR场景中得到了广泛的应用。HandSee\cite{Yu:2019:HEF:3290605.3300935}利用手机上的内置摄像头与三棱镜来捕获用户交互时的手部行为，并借助屏幕与三棱镜的镜面反射特性构建双目系统，重构手部深度图像，并基于此构建一系列交互应用。

同时，手部跟踪是计算机视觉研究的一个重要热门课题，在这个领域中，研究的主要内容为基于模型的算法和基于机器学习的算法。基于模型的方法依赖于传统的模型优化和匹配算法，并且需要结合广泛的领域知识，例如手运动模型\cite{tang2015sampling_optimization}\cite{xu2016articulated}，通过空间和时间特征对手部动作进行预测\cite{sinha2016robust_hand}，手部骨架匹配\cite{de2008model}等等。基于机器学习的算法包括SVM\cite{RealtimeHO_ECCV2016}，随机决策森林\cite{5995316}，高斯混合模型 \cite{Sridhar2016}，多层感知机与卷积神经网络\cite{Zimmermann2017}\cite{Yuan2017}等，这些工作普遍的思路是直接或间接地对手部的21个关键点（MCPs，PIPs，DIPs和TIPs）\cite{DBLP:journals/corr/YuanYSJK17}（如图\ref{fig:hand_keypoints}）进行回归。这些方法以深度图像、RGB图像或者两者结合作为输入，以手部的21个关键的关节点的位置作为输出，输出的手部关键点能精确地刻画手部形状。

\begin{figure}[h]
  \centering
  \includegraphics[width=3in]{figures/related_work/hand.png}
  \caption{手部关键关节点示意图}
  \label{fig:hand_keypoints}
\end{figure}

\section{图像分类与物体检测}

上述基于视觉的手部识别追踪技术大多适用于远场第三视角的手部图像，而对于类似HandSee\cite{Yu:2019:HEF:3290605.3300935}所捕获图像视角的近场手部图像的鲁棒追踪，则显得尤为困难，且目前为止没有十分有效的解决方案。其难度主要体现与两点：一是不完整性与不确定性，近场视角捕获的手部图像存在严重的局部缺失问题，呈现在图像内的往往不是一个完整的手而是手的一部分，不确定手的哪一部分会出现在屏幕中也让这个问题变得更难解决；二是遮挡现象严重，当发生遮挡时信息严重缺失，手部姿态的推理困难。在这种情况下，追求完整而鲁棒的手部关键点信息是不现实的。此时可以“退而求其次”，从图像中挖掘出诸如握姿类别信息、指尖位置信息等与图像分类、物体检测相关的信息。下面我们来回顾一下使用卷积神经网络进行图像分类与物体检测的相关工作。

基于卷积神经网络的图像分类是深度学习领域最经典的任务之一。1998年Yann Lecun首次提出神经网络中的卷积操作，以此构建卷积神经网络LeNet5\cite{Lecun1998}，并将它应用于MNIST手写数字分类任务上。自从2012年AlexNet\cite{Krizhevsky:2012:ICD:2999134.2999257}在ImageNet竞赛上大放异彩，卷积神经网络从此成为图像分类任务的主流方法。随后人们对卷积神经网络架构的可能性进行了长时间的探索，并提出了诸如VGG\cite{2014arXiv1409.1556S}、ResNet\cite{2015arXiv151203385H}、DenseNet\cite{2016arXiv160806993H}等经典网络架构。其中，ResNet在神经网络中首次引入残差，使得梯度能在数百层的网络中进行反向传播\cite{2015arXiv151203385H}；DenseNet进一步演绎与扩展ResNet的残差思想\cite{2016arXiv160806993H}，提出了一种密集连接的模块化结构DenseBlock。同时，也有一系列工作研究如何构建轻量化的卷积神经网络模型，使其通过较低的计算量达到较好的分类效果，以满足移动计算、轻量化计算的需求，MobileNet\cite{2017arXiv170404861H}、MobileNetV2\cite{2018arXiv180104381S}和SqueezeNet\cite{2016arXiv160207360I}就是其中的典型代表。其中，MobileNet首次提出将传统的卷积层分解为深度卷积和点卷积两个运算，极大地提升了计算效率与减少了参数数量；MobileNetV2在MobileNet的基础上引入了类似ResNet的反残差层，使得轻量化的网络也能进行深层计算；SqueezeNet则通过点卷积和退火模块大幅度压缩参数数量。

与图像分类任务相比，物体检测任务显得更有挑战性。对于一个图像，物体检测模型需要输出该图像中所有属于特定类别的物体的矩形包围框区域。早期基于CNN的物体检测方法需要进行大量的候选框扫描，计算量较大；RCNN\cite{2013arXiv1311.2524G}首次提出图像层次化聚类分割的方法，将候选区域数量缩小至2000量级；随后Fast RCNN\cite{2015arXiv150408083G}、Faster RCNN\cite{2015arXiv150601497R}进一步对重复计算、候选框的选取策略等进行优化；紧接着，Yolo\cite{2015arXiv150602640R}、SSD\cite{2015arXiv151202325L}等Single-Shot模型被陆续提出，图像仅需进行一次前向传播就可得到所有的候选框结果，这极大地提升了物体检测模型的计算效率。如今，基于MobileNet\cite{2017arXiv170404861H}、MobileNetV2\cite{Sandler_2018}的SSD物体检测模型被广泛的应用于移动设备。