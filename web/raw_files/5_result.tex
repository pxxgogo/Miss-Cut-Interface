\chapter{实验结果与分析}

\section{计算效率}

我们首先对算法流程的计算效率进行了测试。在真实的硬件条件下（Galaxy S9+，CPU 2.8GHz，8核心，ARM架构），算法流程中每个步骤的用时如表\ref{tbl:alg_speed}所示（单位：毫秒）。

\begin{table}[htbp]
\centering
\caption{算法流程效率}
\label{tbl:alg_speed}
\begin{tabular}{p{40 pt}p{50 pt}p{70 pt}p{70 pt}p{70 pt}p{40 pt}}
 \toprule
  & 握姿分类 & RGB指尖识别 & IR指尖识别 & 下降模式检测 & 总用时\\
 \midrule
单线程 & 66.1 & 35.1 & 13.3 & 2.5 & 103.7 \\
多线程 & 16.5 & 11.0 & 13.3 & 2.5 & 32.3 \\
\bottomrule
 \end{tabular}\\[2pt]
\end{table}

由以上数据可知，我们的算法在移动设备上在多线程计算的条件下可达到30fps的帧率，这说明我们的算法流程能够满足在移动设备上实时计算的需求。我们对比了我们的流程与HandSee的流程的计算量，可以发现，HandSee的计算量主要集中在预处理（13ms）、双目视觉匹配（10ms）、CNN手部分割（4ms）三个步骤，仅这三个步骤在服务器端计算已需花费27ms的时间。除此之外，HandSee的算法流程直接得到的是手部深度图的重构信息与通过简单形态学检测找出的指尖信息，而我们的算法流程可以得到包括握姿、优化的指尖、下降模式等更高级的语义信息。因此，我们的算法流程无论是从效率还是科学性上看，均有较好的表现。

为了说明我们在模型选择时的一些权衡，我们对比了四种经典神经网络模型MobileNetV2、DenseNet161、ResNet152、SqueezeNet 在握姿7分类的任务上分别运算于Nvidia GTX 1080Ti与2.6Ghz单线程Intel x86 CPU的效率，比较结果如表\ref{tbl:nn_speed}所示（单位为毫秒）。由以上结果，我们可以看出，DenseNet与ResNet作为高准确率模型的代表，其模型结构复杂、参数量与计算量大，在CPU下几乎无法支持实时计算；SqueezeNet1.1虽然计算效率极高，但是性能相对较差，不太适合作为SSD的前馈网络；MobileNetV2在兼顾性能与准确率下表现较好。

\begin{table}[htbp]
\centering
\caption{不同神经网络模型效率比较}
\label{tbl:nn_speed}
\begin{tabular}{p{80 pt}p{100 pt}p{100 pt}}
 \toprule
   & Nvidia GTX 1080Ti & x86 CPU @ 2.6Ghz\\
 \midrule
    DenseNet161 & 27.9 & 139.1 \\
    ResNet152 & 23.9 & 369.8 \\
    SqueezeNet1.1 & 2.6 & 6.4 \\
    MobileNetV2 & 5.8 & 54.3 \\
\bottomrule
 \end{tabular}\\[2pt]
\end{table}


\section{握姿分类}

我们分别在5分类RGB数据集、7分类RGB数据集、5分类IR数据集上测试了MobileNetV2、DenseNet161、ResNet152和SqueezeNet1.1四种模型握姿分类的准确率，准确率如表\ref{tbl:ges_class}。

\begin{table}[htbp]
\centering
\caption{握姿分类正确率}
\label{tbl:ges_class}
\begin{tabular}{p{80 pt}p{100 pt}p{100 pt}p{100 pt}}
 \toprule
   & IR @ 5 classes & RGB @ 5 classes & RGB @ 7 classes\\
 \midrule
    DenseNet161 & 99.65\% & 98.88\% & 99.19\% \\
    ResNet152 & 97.23\% & 98.31\% & 96.77\% \\
    SqueezeNet1.1 & 95.96\% & 94.94\% & 95.56\% \\
    \textbf{MobileNetV2} & \textbf{96.89\%} & \textbf{96.63\%} & \textbf{97.98\%} \\
\bottomrule
 \end{tabular}\\[2pt]
\end{table}

由以上结果可知，MobileNetV2模型在RGB图像5分类、RGB图像7分类、IR图像5分类三个任务上都有较好的表现，准确率均大于96.5\%。虽然与DenseNet161仍有一定的差距，但是综合考虑模型的计算量与精度等因素，该模型保证了计算量能够满足移动实时计算的同时，也保持了较高的精度，同时保证了后续SSD模块的有效性，是一个较为合理的选择。

\section{RGB指尖检测}

在这部分，我们将会测试在COCO格式带指尖标注的RGB数据集上我们的指尖检测模型的效果。我们在此给出每个epoch验证集上的平均准确率均值（mAP）与平均召回率（AR）。对于平均准确率均值，我们感兴趣的指标包括mAP、mAP@0.5IOU、mAP@0.75IOU、mAP small、mAP medium和mAP large。其中mAP、mAP@0.5IOU、mAP@0.75IOU分别指在1、0.5、0.75IOU（IOU指对于两个区域，它们的交区域与它们的并区域的比值）下的平均准确率均值；mAP small、mAP medium和mAP large指小、中、大候选区域下的平均准确率均值。具体结果如表\ref{tbl:fingertip_precision}。

\begin{table}[htbp]
\centering
\caption{平均准确率均值}
\label{tbl:fingertip_precision}
\begin{tabular}{p{60 pt}p{60 pt}p{60 pt}p{60 pt}p{60 pt}p{60 pt}}
 \toprule
  mAP & mAP@0.5 & mAP@0.75 & mAP@S & mAP@M & mAP@L\\
 \midrule
    0.73 & 0.98 & 0.90 & 0.63 & 0.77 & 0.85 \\
\bottomrule
 \end{tabular}\\[2pt]
\end{table}

对于平均准确率均值，我们感兴趣的指标包括AR@1、AR@10、AR@100、AR@100S、AR@100M和AR@100L。其中AR@1、AR@10、AR@100分别指置信度最高的前1、10、100个候选框的平均召回率；AR@100S、AR@100M、AR@100L分别指小、中、大候选区域下的平均召回率。具体结果如表\ref{tbl:fingertip_recall}。

\begin{table}[htbp]
\centering
\caption{平均召回率}
\label{tbl:fingertip_recall}
\begin{tabular}{p{60 pt}p{60 pt}p{60 pt}p{60 pt}p{60 pt}p{60 pt}}
 \toprule
  AR@1 & AR@10 & AR@100 & AR@100S & AR@100M & AR@100L\\
 \midrule
    0.56 & 0.78 & 0.78 & 0.70 & 0.83 & 0.88 \\
\bottomrule
 \end{tabular}\\[2pt]
\end{table}

由以上结果可知，在0.75IOU的情况下，平均准确率均值可达0.9，即检出的候选框在0.75IOU下的正确率可达到90\%；置信度前10的候选框可以召回图像中78\%的指尖区域。一个观察是：对于较小的区域（例如持握手四指区域），准确率与召回率均低于中型与大型区域（例如持握手拇指、操作手手指）。这可能是由于持握手四指区域不完整而且没有显著的区分性特征，较容易混淆。对于主要的手指区域（持握手拇指、操作手手指），上述的结果表明我们的模型在这些区域上的可用性较好。一个输出示例如图\ref{fig:rgb_example}所示。

\begin{figure}[h]
  \centering
  \includegraphics[width=5in]{figures/appendix/individualImage.png}
  \caption{RGB指尖检测结果示例}
  \label{fig:rgb_example}
\end{figure}

\section{IR指尖检测}

\begin{table}[htbp]
\centering
\caption{IR指尖检测准确率}
\label{tbl:fingertip_ir_result}
\begin{tabular}{p{60 pt}p{60 pt}p{60 pt}p{60 pt}p{60 pt}p{60 pt}}
 \toprule
   指尖数 & 检出指尖数 & 正确指尖数 & 准确率 & 召回率 & F1 \\
 \midrule
    249 & 231 & 217 & 93.94\% & 87.15\% & 0.904 \\
\bottomrule
 \end{tabular}\\[2pt]
\end{table}

对于IR指尖检测，我们在IR数据集上运行我们的指尖检测算法，在生成的实验结果中随机抽取100帧进行人工标注，标注结果如表\ref{tbl:fingertip_ir_result}。


\begin{figure}[h]
  \centering
  \includegraphics[width=6in]{figures/experiment/fingertip_hist.png}
  \caption{基于曲率直方图特征指尖检测的匹配示例}
  \label{fig:fingertip_hist}
\end{figure}

由以上结果可知，对于IR图像，我们的指尖检测算法准确率可达93.94\%，表现较好。我们观察了检测失败的一些负例，发现检测失败的图像普遍具有以下特点（之一）：由于阴影、遮挡或运动模糊等问题造成的手部轮廓检出不完整；手部轮廓在透视视角下发生遮挡或粘连；指尖位于轮廓区域内部，且轮廓无明显特征。我们希望在后续工作中，尝试解决上述三个问题。但整体而言，单目IR图像下的多指尖检测效果较好，检测质量普遍优于RGB指尖检测。

我们观察了基于曲率分布特征的指尖检测的中间结果，如图\ref{fig:fingertip_hist}所示，左下角为原始图像与检出的轮廓，全图为曲率特征在不同采样频率下的分布。可以看出，在曲率特征度量下，指尖点、指缝点以及全手的形态特征均能得到较好的表示。这也进一步论证了我们的方法的有效性。一个输出示例如图\ref{fig:ir_example}所示，其中，输出的信息包括，每个区域的分类（左、右持握手拇指、持握手四指、操作手、Hand-To-Hand）与区域的指尖。

\begin{figure}
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[height=2in]{figures/experiment/class_1.png}
\end{minipage}%
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[height=2in]{figures/experiment/class_2.png}

\end{minipage}
\caption{IR图像指尖检测结果示例}
\label{fig:ir_example}
\end{figure}


\section{误触检测}
在这部分，我们将会汇报我们在误触检测的下降检测与触点检测这两个步骤的实验结果。

\subsection{下降检测}

对于单手拇指操作、双手拇指操作、单手持握另一只手操作三种握姿对应的预测一致性的人工标注，正确数目、错误数目与正确率如表\ref{tbl:fingertip_movement}。

\begin{table}[htbp]
\centering
\caption{指尖下降检测准确率}
\label{tbl:fingertip_movement}
\begin{tabular}{p{130 pt}|p{60 pt}p{60 pt}p{60 pt}}
 \toprule
    & 正确帧数量 & 错误帧数量 & 正确率\\
 \midrule
    单手拇指操作 & 152 & 10 & 93.83\% \\
    双手拇指操作 & 92 & 7 & 92.93\% \\
    单手持握另一只手操作 & 93 & 13 & 87.74\% \\
\midrule        
    总计 & 337 & 30 & 91.83\% \\
\bottomrule
 \end{tabular}\\[2pt]
\end{table}

由以上结果可知，对于单手拇指与双手拇指操作的情况，下降检测的正确率分别可达93.83\%和92.93\%；单手持握另一只手操作的情况下，由于图像中手部行为与指尖行为更复杂，正确率稍有降低，但仍可以达到87.74\%。三种情况的总计正确率可达91.83\%，这说明我们的下降检测算法能十分有效地检出指尖下降行为。

\subsection{触点检测}
对于触点检测，在真实落点的对比触点检测下，我们的方法正确率可以达到99.3\%；在模拟落点的独立触点检测下，我们的方法正确率可达90.8\%。这说明对于单手操作产生的边缘误触触点，我们的算法几乎可以完全消除这一类误触点；对于模拟干扰信号产生的误触点，我们的算法大概率（大于90\%）可将其消除。

\section{Hand-Around检测}
对于Hand-Around检测，我们的方法在Hand-On/Hand-Around/Hand-Off 3分类任务上正确率可达94.36\%；在Hand-On/Hand-Off 2分类任务上正确率可达99.37\%。这说明当我们拥有朝向手部视角的图像信息后，能够十分有效地实现用户手部Hand-On/Hnad-Around/Hand-Off三个模态的感知与分类。高准确率的Hand-Around分类信息可直接应用于交互中，增强用户的交互体验。

\section{用户实验}

图\ref{fig:mistouch}展示了用户在七种误触场景下对发生频率、体验影响、效率影响、愿意使用四个维度进行的评分，其中1-7分别对应\ref{MistouchTable}中对应编号的场景。

\begin{figure}[h]
  \centering
  \includegraphics[width=6in]{figures/experiment/mistouch.png}
  \caption{日常误触场景调查}
  \label{fig:mistouch}
\end{figure}

结果显示，对于我们提及的7种误触场景，发生频率的平均得分为3.86，最高得分为4.6，这说明这7种误触场景发生频率较频繁，平均每周内都会时有发生，其中“湿手操作手机不灵敏”与“单手‘够’屏幕够不着的位置”两种情况发生频率最高。除此之外，结果显示，7种误触场景在不同程度上对用户的体验与效率造成了一定影响，且用户们均希望（5分）这些误触问题能够得到解决。

对于小球消除任务，5名实验者在误触模式下的平均消除个数为152，平均误触次数为21，平均主观愉悦度为2.2；在误触消除模式下，平均消除个数为198，平均主观愉悦度为4.8。这说明对于单点点击任务，频繁的误触对操作效率与主观愉悦度均有较大的影响，这体现了我们的误触检测应用的价值。

关于现有解锁方式效率，有50\%（10名）的实验者认为现有解锁方式能感觉延迟，但是仍能接受；有25\%（5名）的实验者认为现有解锁方式有明显的延迟，且这种延迟已经对操作体验造成了不同程度的影响。其中后者日常使用的解锁方式包括密码解锁与面部解锁两种，密码解锁的延迟来源于繁琐的操作过程，面部解锁的延迟来源于相对较高（与指纹相比）的失败率与失败代价（失败若干次需使用密码解锁）。我们还调查了实验者对于Hand-Around不锁定应用的观点\ref{HandAroundOp}的认可程度。结果表明，用户们普遍愿意接受这样一项技术（4.85分），对于上述认为现有解锁方式有明显的延迟的实验者，他们的平均接受程度更是高达7分，但是一个普遍的担忧是这项技术的安全性问题（5.35分）。实验者认为，这项技术会在不同程度上提升自己的交互体验与交互效率，在密码解锁（4.65）与面部解锁（4.35）场景下体验与效率提升更明显，而在指纹解锁（3.5）上体验与效率提升相对有限。除此之外，实验者表示，对于Hand-Around显示关键信息的功能，他们更希望从锁屏界面中获取天气、实时新闻、体育赛事等不涉及隐私的信息。